az login

az group create --name trucnv-resource-group --location westus --verbose

az network nsg create --resource-group trucnv-resource-group --name udacity-vmss-nsg --verbose

az vmss create --resource-group trucnv-resource-group --name udacity-vmss --image Ubuntu2204 --vm-sku Standard_B1s --nsg udacity-vmss-nsg --subnet udacity-vmss-vnet-subnet --vnet-name udacity-vmss-vnet --backend-pool-name udacity-vmss-bepool --storage-sku Standard_LRS --load-balancer udacity-vmss-lb --custom-data cloud-init.txt --upgrade-policy-mode automatic --admin-username udacityadmin --generate-ssh-keys --verbose

az network vnet subnet update --resource-group trucnv-resource-group --name udacity-vmss-vnet-subnet --vnet-name udacity-vmss-vnet --network-security-group udacity-vmss-nsg --verbose

az network lb probe create --resource-group trucnv-resource-group --lb-name udacity-vmss-lb --name tcpProbe --protocol tcp --port 80 --interval 5 --threshold 2 --verbose

az network lb rule create --resource-group trucnv-resource-group --name udacity-vmss-lb-network-rule --lb-name udacity-vmss-lb --probe-name tcpProbe --backend-pool-name udacity-vmss-bepool --backend-port 80 --frontend-ip-name loadBalancerFrontEnd --frontend-port 80 --protocol tcp --verbose

az network nsg rule create --resource-group trucnv-resource-group --nsg-name udacity-vmss-nsg --name Port_80 --destination-port-ranges 80 --direction Inbound --priority 100 --verbose

az network nsg rule create --resource-group trucnv-resource-group --nsg-name udacity-vmss-nsg --name Port_22 --destination-port-ranges 22 --direction Inbound --priority 110 --verbose

az monitor app-insights component create --resource-group trucnv-resource-group --app trucnvAppInsights --location westus



# Find the port for connecting via SSH
az vmss list-instance-connection-info --resource-group trucnv-resource-group --name udacity-vmss 

# Connect you to your VM
ssh -p 50001 udacityadmin@20.245.171.133

# Clone and navigate inside the project repo. 
git clone https://github.com/trucvip123/nd081-c4-azure-performance-project-starter.git

cd nd081-c4-azure-performance-project-starter/

# Make sure, you aer in the master branch
git checkout Deploy_to_VMSS

# Update sudo
sudo apt update

# Optional
sudo apt-get install python3-pip

# Install pip
sudo -H pip3 install --upgrade pip

# Install and start Redis server. Refer https://redis.io/download for help.
wget https://download.redis.io/releases/redis-6.2.4.tar.gz
tar xzf redis-6.2.4.tar.gz
cd redis-6.2.4
make

# Ping your Redis server to verify if it is running. It will return "PONG"
redis-cli ping
# The server will start after make. Otherwise, use
src/redis-server     
# Install dependencies - necessary Python packages - redis, opencensus, opencensus-ext-azure, opencensus-ext-flask, flask
cd ..      
pip install -r requirements.txt
# Run the app from the Flask application directory
cd azure-vote/
python3 main.py



## Deploy to AKS
# Creating AKS cluster
az aks create --resource-group trucnv-resource-group --name udacity-cluster --node-count 1 --enable-addons monitoring --generate-ssh-keys
OR
az aks create --resource-group trucnv-resource-group --name udacity-cluster --node-count 1 --generate-ssh-keys
# Connect to AKS cluster
az aks get-credentials --resource-group trucnv-resource-group --name udacity-cluster --verbose

kubectl get nodes

# Create a Container Registry in Azure to store the image
az acr create --resource-group trucnv-resource-group --name myacr202310 --sku Basic

# Log in to the ACR
az acr login --name myacr202310

# Get the ACR login server name
az acr show --name myacr202310 --query loginServer --output table

# Associate a tag to the local image. You can use a different tag (say v2, v3, v4, ....) everytime you edit the underlying image. 
docker tag azure-vote-front:v1 myacr202310.azurecr.io/azure-vote-front:v1

# Push the local registry to remote ACR
docker push myacr202310.azurecr.io/azure-vote-front:v1

# Verify if your image is up in the cloud.
az acr repository list --name myacr202310 --output table

# Associate the AKS cluster with the ACR
az aks update -n udacity-cluster -g trucnv-resource-group --attach-acr myacr202310

# Get the ACR login server name
az acr show --name myacr202310 --query loginServer --output table

# Deploy the application. Run the command below from the parent directory where the *azure-vote-all-in-one-redis.yaml* file is present. 
kubectl apply -f azure-vote-all-in-one-redis.yaml

kubectl set image deployment azure-vote-front azure-vote-front=myacr202310.azurecr.io/azure-vote-front:v1

# Test the application at the External IP
# It will take a few minutes to come alive. 
kubectl get service

# Check the status of each node
kubectl get pods

# Create an autoscaler
kubectl autoscale deployment azure-vote-front --cpu-percent=5 --min=1 --max=10

# Observe the state of the cluster
kubectl get hpa
